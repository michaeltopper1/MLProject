---
title: "Final Project"
author: "Michael Topper and Danny Klinenberg"
output: 
  pdf_document:
    fig_caption: TRUE
    fig_width: 6
    fig_height: 3
bibliography: references.bib
editor_options: 
  chunk_output_type: console
header_includes:
  \usepackage{float}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,  message = FALSE, cache = TRUE, warning = FALSE, fig.pos = "H")
library(tidyverse)
library(fastDummies)
library(ROCR)
library(class)
library(ggpubr)
library(kableExtra)
library(memisc)
library(expss)
#library(rpart)
#library(partykit)
df = board_games <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-03-12/board_games.csv")
select = dplyr::select
theme_set(theme_bw())
```

```{r data.clean}
danny.dat <- df %>%
  separate(.,mechanic, "mechanic1",sep=",",extra="drop") %>% 
  mutate(expansion1=ifelse(is.na(expansion),0,str_count(df$expansion,",")))

top.ten.designer = danny.dat %>% filter(designer != "(Uncredited)") %>% count(designer, sort = T) %>% top_n(10, n)
top.ten.names = top.ten.designer$designer

#added in a top ten variable
danny.dat = danny.dat %>% mutate(top.ten = as.factor(ifelse(designer %in% top.ten.names, 1, 0)))
danny.dat.use<-danny.dat %>% 
  mutate(expansion1=log(expansion1+1),decade = 10 * (year_published %/% 10), above7 = as.numeric(ifelse(average_rating >= 7, 1 , 0))
         ) %>% 
  select(c(above7,max_playtime,min_age,min_players,max_players,top.ten,mechanic1,users_rated,expansion1,decade)) %>% 
  mutate(above7=as.factor(above7)) %>% 
  drop_na()



#Some mechanics have like 1 observation in them. I'm going to drop convert mechanics with less than 20 into just "other"
cat.mut<-danny.dat.use %>% 
  group_by(mechanic1) %>% 
  summarise(n=n()) %>% 
  filter(n<20)

danny.dat.use <- danny.dat.use %>% 
  mutate(mechanic1 = ifelse(mechanic1 %in% cat.mut$mechanic1, "other",mechanic1))

```


```{r train.test}
set.seed(4)

train<-sample(seq(1,dim(danny.dat.use)[1]), .5*dim(danny.dat.use)[1], replace = FALSE)
test<-sample(seq(1,dim(danny.dat.use)[1])[-train], .5*length(seq(1,dim(danny.dat.use)[1])[-train]), replace = FALSE)


train.dat<-danny.dat.use[train,]
test.dat<-danny.dat.use[test,]

#DO NOT TOUCH THIS UNTIL THE VERY VERY END. 
validation.dat <- danny.dat.use[-c(train,test),]
validation.dat  = validation.dat %>% 
  mutate_if(is.character, as.factor) %>% 
  mutate(above7=as.numeric(above7)-1)


```

# Introduction

Social interaction is on the decline. According to recent a recent study using a nationally representative sample of U.S. adolescents and college students, researchers [@twenge2019less] found that adolescences in the 2010s spent significantly less time socializing with their peers than their previous generation counterparts. Specifically, college-bound high school seniors spent less than an hour a day engaging in in-person social interaction. This decline can largely be attributed to the rise of technology. 


The ubiquity of technology has been linked to stymieing the collaborative benefits that social interaction bring. For instance, one study found that non-digital playtime in children lead to increases in creative thinking [@hitron2018digital]. Therefore, with the tech giants constantly battling each other for every second of our attention, it is imperative that human-on-human interaction still plays a major role in everyday life. One of the classic ways to achieve this important interaction is with board games.

Board games suffered a significant reduction in sales upon the spread of the smartphone. However, this trend has recently been reversed [@jolin_2016]. To aid this reverse, we sought to predict whether board games will be rated highly by their users. This information would be crucial so that the board game developers can remain relevant, and human interaction can be salvaged. To answer whether we can predict "what makes a good board game" we utilized four different machine learning techniques: logistic regression, k-nearest neighbors, decision trees, and random forests. Our main results found that 

# Data

Our data set on board games comes from the Board Game Geek database. This data set was published on the R for Datascience Github page for their Tidy Tuesday-a weekly event in which a new public data set is posted for any user to perform analysis on. In particular, this data set features games with at least 50 ratings for games published between the years 1950 and 2016. Surprisingly, this results in 10,532 games. The data features information on the following important variables: a description of the game, maximum number of plays, maximum playtime, minimum age, minimum playtime, year published, artist used for the game art, category of the game, family of the game, average rating, number of users rated, and publisher. Overall, the raw data possesses 22 variables, although we only performed machine learning techniques using 9 covariates. Many of the variables were duplicated into "new" variable names, creating the illusion of more information than was truly available. For instance, there were two variables that included identical information but were labeled differently as "max playtime" and "playing time".  We excluded the duplicates from the analysis. 

## Exploratory Analysis

Before building our machine learning models, we first performed exploratory analysis. To begin, we plotted several scatter and box-and-whisker plots to visualize the relationship between our covariates and whether the game received a high average score. 

We first will highlight four main features: *Maximum Playtime (minutes)*, *Users Rated*, *Decade*, and *Number of Expansions*. We chose to use maximum playtime over minimum or average because we thought it was the most representative of the game playing experience. In addition, minimum and average are expected to be highly correlated with maximum so including one would suffice for predictive power. *User Rated* refers to the number of individuals that rated the game. This serves as a proxy for popularity of the game. *Decade* was a transformed variable from *date released*. We assumed there were some sort of generational fixed effects involved in the evolution of board games hopefully captured by *Decade*. Finally, we transformed *Expansions* to *Number of Expansions*. Good games tend to have more expansions than bad games so the magnituded was an appealing feature. The relationship was initially exponential; a few games had lots of expansions and were rated very high. To account for this, the number of expansions was logged. This created a linear relationship for analysis.

```{r features_graphs_maker}
playtime<-df %>% filter(max_playtime < 10000) %>% 
  ggplot(aes(x = max_playtime, y = average_rating)) +
  geom_point(color = 'grey') +
  geom_smooth( se = F) +
  geom_hline(yintercept = 7, color = 'red') +
  xlab("Maximum Playtime (minutes)") +
  ylab("")+
  theme(plot.title = element_text(hjust = 0.5)) 

user_rate<-df %>% ggplot(aes(x = users_rated, y = average_rating)) +
  geom_point(color = 'grey') +
  geom_smooth(se = F) +
  geom_hline(yintercept = 7, color = 'red') +
  xlab("Users Rated") +
  ylab("")+
  theme(plot.title = element_text(hjust = 0.5)) 

year <- df %>% mutate(decade = 10 * (year_published %/% 10)) %>% 
  group_by(decade) %>% summarise(min_average_players = mean(min_players), avgdecaderating = mean(average_rating)) %>%
  ggplot(aes(x = decade, y = avgdecaderating)) + 
  geom_point() + 
  geom_smooth(method = 'lm', se = F) +
  xlab("Decade") +
  ylab("")+
  theme(plot.title = element_text(hjust = 0.5))  

expansion <- danny.dat %>%
  ggplot(., aes(I(log(expansion1 + 1)), average_rating)) +
  geom_point() +
  geom_smooth(se = F) +
  ylab("")+
  xlab("ln(Number Expansions)") 

```

```{r, fig.cap = "Relationship of Sample Features", fig.width=6, fig.height=3}
annotate_figure(ggarrange(playtime,user_rate,year, expansion),
  left = text_grob("Average Ratings", rot = 90)
)
```


```{r decade_graph, fig.width=6, fig.height=3, include=FALSE}
df %>% mutate(goodrating = as.factor(ifelse(average_rating >= 7, 1, 0))) %>% 
  ggplot(aes(x = goodrating, y = min_age, fill = goodrating)) +
  geom_boxplot() +
  labs(title = "Minimum Age and Rating", fill = "1 if rating above 7") +
  xlab("Classifier of Good or Bad Game") +
  ylab("Minimum Age Requirement") +
  theme(plot.title = element_text(hjust = 0.5)) 
```


```{r}
mechanics = df %>% separate_rows(mechanic, sep = ",") %>% select(mechanic, game_id, publisher)
```



In addition, we also created several other graphs to get a better feel for the data. Some of the questions we were interested in answering were: 

\begin{enumerate}
    \item \textit{Were there any popular designers in each decade and did these designers have an effect on the rating?}
    \item \textit{What were the most popular types of board games and does type effect the rating?}
\end{enumerate}

```{r designers, fig.cap="Top 5 Designers by Decade", fig.width=6, fig.height=7}
df %>% mutate(decade = 10 * (year_published %/% 10)) %>% 
  filter(decade > 1950) %>% 
  filter(designer != '(Uncredited)') %>% 
  group_by(decade) %>% 
  count(designer, sort = T) %>%  
  filter(!is.na(designer)) %>% 
  arrange(desc(n)) %>% 
  top_n(5, n) %>% 
  ggplot(aes(x = designer, y = n, fill = as.factor(decade) )) +
  geom_col() +
  facet_wrap(~decade) +
  coord_flip() +
  labs(fill = "Decade") +
  xlab("") +
  ylab("Count")

```

We look at the types of games included. The majority of games were *Card Games*, followed by *Wargames*, and *Fantasy*. An initial concern is that there are many games that have very specific types. For example, *Connection Games* has 19 observations. This will lead to issues in cross validation. Because of this, we group all categories under a threshold as *Other*.

```{r pop_type, fig.cap="Most Popular Types of Board Games", fig.width=6, fig.height=3}
catvars = df %>% 
  select(game_id, name, family, expansion, category, artist, designer, average_rating) %>% 
  gather(type, value, -game_id, -name) %>% 
  filter(!is.na(value)) %>% 
  separate_rows(value, sep = ',') %>% 
  arrange(game_id)

catcounts = catvars %>% 
  count(type, value,  sort = T)

catcounts %>% 
  filter(type == 'category') %>% 
  mutate(value = fct_reorder(value, n)) %>% 
  top_n(20, n) %>% 
  ggplot(aes(value, n)) +
  geom_col(fill = 'blue') +
  coord_flip() +
  ylab("Total") +
  xlab("Board Game Type") +
  theme(plot.title = element_text(hjust = 0.5)) 
```


## Final Predictors

For our final analysis, we utilized 9 final predictors. The predictors are a mix of both categorical and numerical data:
\begin{enumerate}
    \item Maximum playtime
    \item Minimum age
    \item Minimum players
    \item Maximum players
    \item Whether the game was designed by a ``top ten'' designer
    \item Game mechanics
    \item Whether the game was an expansion
    \item Decade the game was published
    \item Number of users who rated the game
\end{enumerate}
We decided on these predictors due to the relationships we saw in our exploratory analysis. Furthermore, the other variables were either too sparse or required advanced methods to clean. For instance, our compilation predictor which included information on whether or not the board game was part of a compilation series, only had information for 337 of our 10,000 observations. Therefore, we dropped this variable from analysis. Moreover, the variables "description", "image", and "thumbnail" required image processing techniques in which we believed were out of the scope of this course.

We created two new variables of the nine that were used as final predictors: a binary variable for whether the game was designed by a top ten designer, and decade the game was published. Originally, the data had information on the designer that created each game. To create the binary variable, we first filtered the data to make sure there were no uncredited games. Next, we counted the top designers and sorted them in descending order. We took the top 10 designers by the number of games they designed--if there was a tie (e.g. the 10th and 11th designer had the same count) we included them both as a top ten designer. For the decade variable, we simply grouped our year data into decades. This was accomplished because we hypothesized that games changed more by decade than by year. 

# Methods

To begin, we first made a classifier for whether a game had an average user score of a 7 or over on a 1-10 scale. This decision was made because 7 was approximately the 80th percentile of the distribution of scores. In addition, we also made a classifier for whether a game had an average user score of 5.7 or below. This number was roughly the 20th percentile of our data. After defining these classifiers, we performed logistic regression, k-nearest neighbors, and decision trees as our non-ensemble methods. For ensemble methods, we used random forests and bagging. More detailed explanations on how these were performed follows.

 To ensure replication of results, we set the seed to 4. We split the data into a training, testing, and validation set. The splits were made randomly on a 50-25-25 split respectively. 
 
## Logistic Regression

To begin, we run a logistic regression using the variables described above. The training rate versus cutoff is displayed below. The formula is:

$$Pr(Above=1|X)=\frac{e^{X^T\beta}}{1+e^{X^T\beta}}$$

```{r}
mod_logit<-glm(above7~.,data = train.dat, family = binomial(link = "logit"))
summ<-summary(mod_logit)
a<-summ$coefficients

```

where $X^T$ is the list of variables described before. Highly significant features in the training set (defined as having a p-value less than .001) are: **`r row.names(a)[which(a[,4]<.001)]`**. A full regression breakdown can be found at the end of the analysis.




```{r train.logit}
y.hat <- predict(glm(above7~.,data = train.dat, family = binomial(link = "logit")), type = "response")
cutoff <- function(p){
  return(as.numeric(y.hat > p))
}
```

The predicted values of a logistic regression are probabilities. This means that a cutoff has be chosen for *above7*. An immediate thought may be 50%. That way half the values are below or above. However, the cutoff should be chosen to minize the error rate. Below is a visual representation of accuracy versus cutoff values for the training set.

```{r train.cuttoff.graph, fig.cap="Cutoff vs % Correct: Training", fig.width=6, fig.height=3}
hold <- tibble("cutoff" = seq(0,1,.01),
             "% correct" = rep(NA, length(seq(0,1,.01))))

for (i in seq_len(length(seq(0, 1, .01)))) {
  hold[i,2] <- mean(cutoff(hold$cutoff[i]) == train.dat$above7)
}

ggplot(hold, aes(x = cutoff, y = `% correct`)) +
  geom_line()

```

Notice that the accuracy rate peaks between 25% and 50%. This means that we expect the cutoff rate for the test set to be near the same.

Now, we will check the optimal cutoff for the test set using the model created by the training model.


```{r Logit_ROC_AUC, fig.cap="ROC Curve", include=FALSE}
glm.fit <- glm(above7~.,data = train.dat, family = binomial(link = "logit"))
prob.training <- predict(glm.fit, type = "response")

prob.test <- round(predict(glm.fit, test.dat, type = "response"),digits = 5) 



pred <- prediction(prob.training, train.dat$above7)
perf <- performance(pred, measure = "tpr",x.measure = "fpr")
plot(perf, col = 2, lwd = 3)
abline(0,1)
```

A useful way of determining the quality of a fit is the ROC curve. The ROC curve shows the Type 1 and Type II errors of each cutoff for a logistic regression.


```{r AUC_val}
#AUC value
auc <- performance(pred, "auc")@y.values
```

The area under the curve is `r round(auc[[1]],3)`. The perfect fit would have an area under the curve of 1. This would mean the model perfectly predicts. Anything above .9 is considered great and anything above .75 is considered good. This is used to tell us if our model does good in general. Now that we know in general the model performs well, we can pick the optimal cutoff.

Optimal cutoff is chosen by minimizing the type I and type II error. A useful graph is provided below:

```{r, fig.cap="Test Error vs Threshold"}
# Obtaining best cutoff value:

fpr = performance(pred, "fpr")@y.values[[1]] 
cutoff = performance(pred, "fpr")@x.values[[1]] # FNR 
fnr = performance(pred,"fnr")@y.values[[1]]

matplot(cutoff, cbind(fpr,fnr), type = "l",lwd = 2, xlab = "Threshold",ylab = "Error Rate") # Add legend to the plot 
#legend("topright",legend = c("False Positive Rate","False Negative Rate"), col = c(1,2), lty = c(1,2))


rate = as.data.frame(cbind(Cutoff = cutoff, FPR = fpr, FNR = fnr)) 
rate$distance = sqrt((rate[,2])^2 + (rate[,3])^2)

index = which.min(rate$distance) 
best = rate$Cutoff[index] 

best.pred <- mean(as.numeric(prob.test > best) == test.dat$above7)
```

The red dotted line is the *false negative* rate while the black solid line is the *false positive* rate. The optimal threshold is chosen by minizing the Euclidean distance of the two curves from 0.

The best value to use as a cutoff is `r round(best,3)` with a  success rate of `r round(best.pred,3)`. This is the best cutoff value using the test set.

## KNN

*K Nearest Neighbors (KNN)* predicts by matching known observations with an observation we want to predict. KNN works very intuitively: if asked what one observation will act live, use those most similar to it. The difficulty of KNN is choosing the optimal number of neighbors. The key here is to minimize the MSE by choosing the optimal number of k. We start by looking just at k=2.

```{r KNN.setup}
Ytrain <- train.dat$above7
Xtrain <- train.dat %>% select(-above7) %>% 
  dummy_cols(.,  remove_selected_columns = TRUE)
Xtrain <- scale(Xtrain,center = TRUE, scale = TRUE)

Ytest <- test.dat$above7
Xtest <- test.dat %>% 
  select(-above7) %>% 
  dummy_cols(., remove_selected_columns = TRUE) %>% 
  scale(center = TRUE, scale = TRUE)

```

```{r KNN.train.2}
pred.ytrain <- knn(train = Xtrain, test = Xtrain, cl = Ytrain, k = 2)

conf.train <- table(predicted = pred.ytrain,observed = Ytrain)
```

```{r KNN.test.2}
pred.ytest <- knn(train = Xtrain, test = Xtest, cl = Ytrain, k = 2)

conf.test <- table(predicted = pred.ytest,observed = Ytest)
```

The training error with 2 neighbors is `r sum(diag(conf.train)/sum(conf.train))`. The true positive rate is `r round(conf.train[1,1]/sum(conf.train[,1]),3)` and the false positive rate is `r round(conf.train[1,2]/sum(conf.train[1,]),3)`.

The test error with 2 neighbors is `r sum(diag(conf.test)/sum(conf.test))`. The true positive rate is `r round(conf.test[1,1]/sum(conf.test[,1]),3)` and the false positive rate is `r round(conf.test[1,2]/sum(conf.test[1,]),3)`.

```{r KNN.cv, cache=TRUE}
validation.error <- NULL
allK <- 1:35
set.seed(66)

for (i in allK) { # Loop through different number of neighbors
  pred.Yval = knn.cv(train = Xtrain, cl = Ytrain, k = i) # Predict on the left-out validation set
  validation.error = c(validation.error, mean(pred.Yval != Ytrain)) # Combine all validation errors 
  }

numneighbor = max(allK[validation.error == min(validation.error)]) 

```

However, the odds of 2 being the correct number of neighbors is very slim. In order ot determine the correct number of neighbors, Leave One Out Cross Validation (LOOCV) is employed. LOOCV works in the following steps:

\begin{enumerate}
    \item Remove one observations as a holdout
    \item Create a KNN model for many k
    \item predict using the models on the one observation holdout
    \item record the error for each model
    \item Repeat for each observation
    \item choose the number of neighbors (k) that minimized average error rate.
\end{enumerate}

This can be easily graphed. However, visually identifying the optimal number of neighbors is near impossible. Instead, we result to identifying the maximium

```{r neibor_graph, fig.cap="Neighbor Graph"}
kn_graph <- tibble(
  "Neighbors" = allK,
  "Error Rate" = validation.error
)
ggplot(kn_graph, aes(x = Neighbors, y = `Error Rate`)) +
  geom_line()

```

The optimal number of neighbors determined using LOOCV is `r numneighbor`. Using the optimal number of neighbors, we can produce the test confusion matrix and test error rate:

```{r KNN.actual}
set.seed(67)

pred.YTest = knn(train = Xtrain, test = Xtest, cl = Ytrain, k = numneighbor)

conf.matrix = table(predicted = pred.YTest, true = Ytest) 

kable(conf.matrix, caption = "Test Confusion Matrix", align = c("c","c"), col.names = c("0","1")) %>% 
  kable_styling(bootstrap_options = "striped",  font_size = 8, latex_options = "hold_position") 
```

KNN returns a test error of `r round(sum(diag(conf.matrix)/sum(conf.matrix)),3)` with `r numneighbor` neighbors. This is approximately the same as the logistic regression.

## Decision Trees

ecision trees were made using the tree library in R. To begin, we built the tree including all predictors. Using this our prediction tree is diplayed in Figure \ref{DecisionTree}.


```{r treegraph, fig.cap="\\label{DecisionTree}Decision Tree on Training Set", fig.width=6, fig.height=4}
library(tree)
tree.fit = tree(above7 ~., data = train.dat)
plot(tree.fit)
text(tree.fit, pretty = 0, cex = 0.7)
title(" ")
```

```{r}
yhat.testset = predict(tree.fit, newdata = test.dat, type = 'class')
#Obtaining a confusion matrix
error.tree = table(yhat.testset, test.dat$above7)
tree.accuracy = sum(diag(error.tree)/sum(error.tree))
tree.error.rate = 1 - tree.accuracy
```
This particular tree did a decent job predicting whether a game was rated highly-- it received a misclassification rate of `r round(tree.error.rate,3)`. 


```{r}
cv.for.tree = cv.tree(tree.fit, FUN=prune.misclass, K=10)
optimal.size = cv.for.tree$size[which.min(cv.for.tree$dev)]
```

```{r prunetree, fig.cap="\\label{Prunetree}Pruned Decision Tree Using Cross Validation for Optimal Size", fig.width=6, fig.height=4}
tree.prune = prune.misclass(tree.fit, best = optimal.size)
plot(tree.prune)
text(tree.prune, pretty = 0, cex = 0.8)
title(" ")

```
```{r}
pred.prune = predict(tree.prune, newdata = test.dat, type = 'class')
error.prune = table(pred.prune, test.dat$above7)
prune.accuracy = sum(diag(error.prune))/sum(error.prune)
prune.error.rate = 1 - prune.accuracy
```



To enhance this tree, we next used cross validation to find the optimal tree size. After performing cross validation, we found that the optimal tree size was `r optimal.size`.  Using this optimal size, we then pruned the tree and once again performed our tree prediction algorithm. When pruning and optimal tree size were included, the test misclassication rate was `r round(prune.error.rate, 3)`. This was extremely similar to our original tree and therefore we can conclude that pruning/cross validation did not lead to a significant increase in performance. Our pruned decision tree is shown in Figure \ref{Prunetree}.

## Random Forest/Bagging

Our main ensemble method used was the random forest. We created a random forest using the randomForest package in R. To begin, we arbitrarily chose a parameter for ``mtry'', the number of variables that should be considered on each iteration when creating each branch in the random forest's trees. When $mtry = 3$ we found that 

Our main ensemble method used was the random forest. We created a random forest using the randomForest package in R. To begin, we arbitrarily chose a parameter for ``mtry'', the number of variables that should be considered on each iteration when creating each branch in the random forest's trees. 
```{r rf, fig.cap="\\label{rf} Random Forest", fig.width=6, fig.height=3}
library(randomForest)
train.dat = train.dat %>% mutate_if(is.character, as.factor)
test.dat = test.dat %>% mutate_if(is.character, as.factor)
rf.tree = randomForest(above7 ~., data = train.dat, mtry = 3, ntree = 500, importance = T)
plot(rf.tree, main = "")
```

```{r}
yhat.rf = predict(rf.tree, newdata = test.dat)
rf.err = table(pred = yhat.rf, truth = test.dat$above7)
rf.test.error = 1 - sum(diag(rf.err))/sum(rf.err)
```


```{r, include = FALSE}
## I don't want this to show up!
imp = varImpPlot(rf.tree)
imp = as.data.frame(imp)
imp$varnames = rownames(imp)
rownames(imp) = NULL
```

```{r importanceplot, fig.cap="\\label{importanceplot}Importance Plot", fig.width=6, fig.height=3}

## I want to put these side by side
mean.accuracy = imp %>% 
  ggplot(aes(x = reorder(varnames, MeanDecreaseAccuracy), y = MeanDecreaseAccuracy)) +
  geom_point() +
  geom_segment(aes(x = varnames, xend=varnames, y = 0, yend = MeanDecreaseAccuracy)) +
  xlab("Variable") +
  ylab("Mean Decrease Accuracy") +
  coord_flip()

mean.gini = imp %>% 
  ggplot(aes(x = reorder(varnames, MeanDecreaseGini), y = MeanDecreaseGini)) +
  geom_point() +
  geom_segment(aes(x = varnames, xend=varnames, y = 0, yend = MeanDecreaseGini)) +
  xlab("Variable") +
  ylab("Mean Decrease Gini") +
  coord_flip()

ggarrange(mean.gini, mean.accuracy)
```


```{r}
# rf.error.loop = rep(0, 8)
# for (i in 2:9) {
#   rf.tree.loop = randomForest(above7 ~., data = train.dat, mtry = i, ntree = 500, importance = T)
#   yhat.rf.loop = predict(rf.tree.loop, newdata = test.dat)
#   rf.err.loop = table(pred = yhat.rf.loop, truth = test.dat$above7)
#   rf.test.error.loop = 1 - sum(diag(rf.err.loop))/sum(rf.err.loop)
#   rf.error.loop[i-1] = rf.test.error.loop
# }
```



When $mtry = 3$ we found that our misclassification rate was `r round(rf.test.error, 3)`. This was a significant upgrade from our previous decision tree method. To find the optimal $m$, we looped over multiple values of $m$. Note that when $m=9$, we are performing bagging. 

## Random Forest/Bagging

Our main ensemble method used was the random forest. We created a random forest using the randomForest package in R. To begin, we arbitrarily chose a parameter for ``mtry'', the number of variables that should be considered on each iteration when creating each branch in the random forest's trees. When $mtry = 3$ we found that 



# Conclusion

We have presented four machine learning methods to predict which board games will score above a 7.5. We first observed the data, identified key variables, and tranformed when necessary. We then split the data into training (50%), testing (25%), and validation (25%). We will now present the validation results and determine an optimal model:

```{r VAlidation_make}
#Logit

logit.valid <- round(predict(glm.fit, newdata = validation.dat, type = "response"),digits = 5) 
logit.pred <- as.numeric(logit.valid > best)
logit_error <- mean((validation.dat$above7 != logit.pred))

#KNN

Yvalid <- validation.dat$above7
Xvalid <- validation.dat %>% 
  select(-above7) %>% 
  dummy_cols(., remove_selected_columns = TRUE) %>% 
  scale(center = TRUE, scale = TRUE)

pred.ytest.val <- knn(train = Xtrain, test = Xvalid, cl = Ytrain, k = numneighbor)
KNN_error <- mean(pred.ytest.val != Yvalid)

# Tree

pred.prune.val = predict(tree.prune, newdata = validation.dat, type = 'class')
error.prune.val = table(pred.prune.val, validation.dat$above7)
prune.accuracy.val = sum(diag(error.prune.val))/sum(error.prune.val)
tree_error = 1 - prune.accuracy.val

# Random Forest

yhat.rf.v = predict(rf.tree, newdata = validation.dat)
rf.err.v = table(pred = yhat.rf.v, truth = validation.dat$above7)
rf_error = 1 - sum(diag(rf.err.v))/sum(rf.err.v)

validation_table <- tibble("Method" = c("Logistic","KNN","Tree","Random Forest"),
                         "Validation Error Rate" = round(c(logit_error,KNN_error,tree_error,rf_error),3)
)

kable(validation_table, caption = "Validation Error Rate", align = c("c","c")) %>% 
  kable_styling(bootstrap_options = "striped",  font_size = 8, latex_options = "hold_position") %>% 
    row_spec(0, bold = T, background = col2hex("deepskyblue4"), color = "white")
```

For this dataset, logistic performed the worst. KNN and tree performed relatively similar while Random Forest was the obvious winner. Given this dataset, we would recommend using Random Forest to predict future quality board games. The major benefit of Random Forest is the ensemble nature: by aggregating many trees the method is able to add precision to out of sample modeling. In conclusions, use Random Forest the next time you want to play a new board game to guarantee it's a banger.

# Appendix

```{r, outcum_hist, fig.cap="Distribution of Average Ratings Across Games"}
df %>% ggplot(aes(average_rating)) +
  geom_histogram(fill = "grey") +
  geom_vline(aes(xintercept = 7), color = 'red') +
  labs(xintercept = "Classification Cutoff") +
  theme(plot.title = element_text(hjust = 0.5))  +
  ylab("Count") +
  xlab("Average Rating") +
  theme_classic()
```

```{r logit_regression}
mtable(mod_logit)
```

# Cited