---
title: "Cleaning"
author: "Michael Topper and Danny Klinenberg"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
library(tidyverse)
library(dplyr)
library(fastDummies)
library(ROCR)
library(class)
df = board_games <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-03-12/board_games.csv")

theme_set(theme_bw())
```


# Exploratory Data Analysis


I have some workds

```{r}
df %>% ggplot(aes(average_rating)) +
  geom_histogram(fill = "grey") +
  geom_vline(aes(xintercept = 7), color = 'red') +
  labs(xintercept = "Classification Cutoff", title = "Distribution of Average Ratings Across Games") +
  theme(plot.title = element_text(hjust = 0.5))  +
  ylab("Count") +
  xlab("Average Rating") +
  theme_classic()

```

I have more words

Now going to make a graph of some possible predictors
```{r}
df %>% filter(max_playtime < 10000) %>% 
  ggplot(aes(x = max_playtime, y = average_rating)) +
  geom_point(color = 'grey') +
  geom_smooth( se = F) +
  geom_hline(yintercept = 7, color = 'red') +
  labs(title = "Maxmimum Playtime and Average Rating") +
  xlab("Maximum Playtime (minutes)") +
  ylab("Average User Rating") +
  theme(plot.title = element_text(hjust = 0.5)) 

df %>% ggplot(aes(x = users_rated, y = average_rating)) +
  geom_point(color = 'grey') +
  geom_smooth(se = F) +
  geom_hline(yintercept = 7, color = 'red') +
  labs(title = "Number of Users Rated and Average Rating") +
  xlab("Users Rated") +
  ylab("Average User Rating") +
  theme(plot.title = element_text(hjust = 0.5)) 

df %>% mutate(decade = 10 * (year_published %/% 10)) %>% 
  group_by(decade) %>% summarise(min_average_players = mean(min_players), avgdecaderating = mean(average_rating)) %>%
  ggplot(aes(x = decade, y = avgdecaderating)) + 
  geom_point() + 
  geom_smooth(method = 'lm', se = F) +
  labs(title = "Average Decade Rating Over Time") +
  xlab("Decade") +
  ylab("Average Rating in the Decade") +
  theme(plot.title = element_text(hjust = 0.5))  

df %>% mutate(goodrating = as.factor(ifelse(average_rating >=7, 1, 0))) %>% 
  ggplot(aes(x = goodrating, y = min_age, fill = goodrating)) +
  geom_boxplot() +
  labs(title = "Minimum Age and Rating", fill = "1 if rating above 7") +
  xlab("Classifier of Good or Bad Game") +
  ylab("Minimum Age Requirement") +
  theme(plot.title = element_text(hjust = 0.5)) 

```



Finding the top types of games that are created. 
```{r}
catvars = df %>% 
  select(game_id, name, family, expansion, category, artist, designer, average_rating) %>% 
  gather(type, value, -game_id, -name) %>% 
  filter(!is.na(value)) %>% 
  separate_rows(value, sep = ',') %>% 
  arrange(game_id)

catcounts = catvars %>% 
  count(type, value,  sort = T)

catcounts %>% 
  filter(type == 'category') %>% 
  mutate(value = fct_reorder(value, n)) %>% 
  top_n(20, n) %>% 
  ggplot(aes(value, n)) +
  geom_col(fill = 'blue') +
  coord_flip() +
  ylab("Total") +
  xlab("Board Game Type") +
  labs(title = "Most Popular Types of Board Games") +
  theme(plot.title = element_text(hjust = 0.5)) 
```






Making a bar graph of decade 
```{r}
df %>% mutate(decade = 10 * (year_published %/% 10)) %>% 
  group_by(decade) %>% summarise(min_average_players = mean(min_players), avgdecaderating = mean(average_rating)) %>%
  ggplot(aes(x = decade, y = avgdecaderating)) + 
  geom_point() + 
  geom_smooth(method = 'lm', se = F) +
  labs(title = "Average Decade Rating Over Time") +
  xlab("Decade") +
  ylab("Average Rating in the Decade") +
  theme(plot.title = element_text(hjust = 0.5))  


```

Designers by decade.
```{r}
df %>% mutate(decade = 10 * (year_published %/% 10)) %>% 
  filter(decade > 1950) %>% 
  filter(designer != '(Uncredited)') %>% 
  group_by(decade) %>% 
  count(designer, sort = T) %>%  
  filter(!is.na(designer)) %>% 
  arrange(desc(n)) %>% 
  top_n(5, n) %>% 
  ggplot(aes(x = designer, y = n, fill = as.factor(decade) )) +
  geom_col() +
  facet_wrap(~decade) +
  coord_flip() +
  labs(fill = "Decade", title = "Top 5 Designers by Decade") +
  xlab("Designer") +
  ylab("Count")

```


Publisher and mechanics
```{r}
mechanics = df %>% separate_rows(mechanic, sep = ",") %>% select(mechanic, game_id, publisher)

mechanics %>% 
  group_by(game_id) %>% count(mechanic, sort = T)
```


# Danny Cleaning

Let's just use the first mechanic the main mechanic and then count the number of expansions each game has
```{r data.make}
danny.dat<-df %>% 
  separate(.,mechanic, "mechanic1",sep=",",extra="drop") %>% #Just grabbing the first mechanic for our purposes 
  mutate(expansion1=ifelse(is.na(expansion),0,str_count(df$expansion,",")))
```
```{r expansion.relationship}
danny.dat %>%
  ggplot(., aes(I(log(expansion1+1)), average_rating))+
  geom_point()+
  geom_smooth()
```
Looks like more is better here. The relationship is a little funky but overall increasing. The relationship looks best as a log relationship. So I'm going to change expansion1 to that.

```{r data.clean}

top.ten.designer = danny.dat %>% filter(designer != "(Uncredited)") %>% count(designer, sort = T) %>% top_n(10, n)
top.ten.names = top.ten.designer$designer

#added in a top ten variable
danny.dat = danny.dat %>% mutate(top.ten = as.factor(ifelse(designer %in% top.ten.names, 1, 0)))
danny.dat.use<-danny.dat %>% 
  mutate(expansion1=log(expansion1+1),
         decade = 10 * (year_published %/% 10),
         above7 = as.numeric(ifelse(average_rating >= 7, 1 , 0))
         ) %>% 
  select(c(above7,max_playtime,min_age,min_players, max_players,top.ten, mechanic1,users_rated,expansion1,decade)) %>% 
  #dummy_cols(., remove_selected_columns = TRUE) %>%  #Couldn't get mechanic1 to register so have to do a trick
  mutate(above7=as.factor(above7)) %>% 
  drop_na()



#Some mechanics have like 1 observation in them. I'm going to drop convert mechanics with less than 20 into just "other"
cat.mut<-danny.dat.use %>% 
  group_by(mechanic1) %>% 
  summarise(n=n()) %>% 
  filter(n<20)

danny.dat.use<-danny.dat.use %>% 
  mutate(mechanic1=ifelse(mechanic1 %in% cat.mut$mechanic1, "other",mechanic1))

```

# Training and Testing

```{r train.test}
set.seed(4)

train<-sample(seq(1,dim(danny.dat.use)[1]), .5*dim(danny.dat.use)[1], replace = FALSE)
test<-sample(seq(1,dim(danny.dat.use)[1])[-train], .5*length(seq(1,dim(danny.dat.use)[1])[-train]), replace = FALSE)


train.dat<-danny.dat.use[train,]
test.dat<-danny.dat.use[test,]

#DO NOT TOUCH THIS UNTIL THE VERY VERY END. 
validation.dat<-danny.dat.use[-c(train,test),]
validation.dat  = validation.dat %>% mutate_if(is.character, as.factor)


```

# Logistic Regression
```{r train.logit}
y.hat<-predict(glm(above7~.,data = train.dat, family = binomial(link="logit")), type="response")
cutoff<-function(p){
  return(as.numeric(y.hat>p))
}

mean(cutoff(.5)==train.dat$above7)
```

```{r train.cuttoff.graph}
hold<-tibble("cutoff"=seq(0,1,.01),
             "% correct"=rep(NA, length(seq(0,1,.01))))

for(i in seq_len(length(seq(0,1,.01)))){
  hold[i,2]<-mean(cutoff(hold$cutoff[i])==train.dat$above7)
}

ggplot(hold, aes(x=cutoff, y=`% correct`))+
  geom_point()+
  ggtitle("Cutoff vs % Correct: In sample")

```

Notice this is the in-sample best value. There is probably some overfitting going on. Now I will check with the test set.


```{r Logit.ROC.AUC}
glm.fit<-glm(above7~.,data = train.dat, family = binomial(link="logit"))
prob.training<-predict(glm.fit, type="response")

prob.test<-round(predict(glm.fit, test.dat, type="response"),digits=5) 

test.dat %>% 
  mutate(Prob=prob.test)

pred<-prediction(prob.training, train.dat$above7)
perf<-performance(pred, measure = "tpr",x.measure = "fpr")
plot(perf, col=2, lwd=3, main="ROC curve")
abline(0,1)

#AUC value
auc<-performance(pred, "auc")@y.values
auc

# Obtaining best cutoff value:

fpr = performance(pred, "fpr")@y.values[[1]] 
cutoff = performance(pred, "fpr")@x.values[[1]] # FNR 
fnr = performance(pred,"fnr")@y.values[[1]]

matplot(cutoff, cbind(fpr,fnr), type="l",lwd=2, xlab="Threshold",ylab="Error Rate") # Add legend to the plot 
legend(0.3, 1, legend=c("False Positive Rate","False Negative Rate"), col=c(1,2), lty=c(1,2))


rate = as.data.frame(cbind(Cutoff=cutoff, FPR=fpr, FNR=fnr)) 
rate$distance = sqrt((rate[,2])^2+(rate[,3])^2)

index = which.min(rate$distance) 
best = rate$Cutoff[index] 
best 

best.pred<-mean(as.numeric(prob.test>best)==test.dat$above7)
```

The best value to use as a cutoff is `r best` with a  success rate of `r best.pred`. This is the best cutoff value using the test set.

# KNN

```{r KNN.setup}
Ytrain<-train.dat$above7
Xtrain<-train.dat %>% select(-above7) %>% 
  dummy_cols(., remove_selected_columns = TRUE)
Xtrain<-scale(Xtrain,center=TRUE, scale = TRUE)

Ytest<-test.dat$above7
Xtest<-test.dat %>% 
  select(-above7) %>% 
  dummy_cols(., remove_selected_columns = TRUE) %>% 
  scale(center=TRUE, scale=TRUE)

```

Set up training success rate.
```{r KNN.train.2}
pred.ytrain<-knn(train=Xtrain, test=Xtrain, cl=Ytrain, k=2)

conf.train<-table(predicted=pred.ytrain,observed=Ytrain)

conf.train

#Train accuracy
sum(diag(conf.train)/sum(conf.train))
```

Now trying with the test set

```{r KNN.test.2}
pred.ytest<-knn(train=Xtrain, test=Xtest, cl=Ytrain, k=2)

conf.test<-table(predicted=pred.ytest,observed=Ytest)

conf.test

#Train accuracy
sum(diag(conf.test)/sum(conf.test))
```

With a nearest neighbor of 2, we can get the test success rate up to `r sum(diag(conf.test)/sum(conf.test))`.

Now, I will do a cross validation to figure out which number of neighbors I should use.  
```{r KNN.cv}
validation.error<-NULL
allK<-1:1
set.seed(66)

for (i in allK){ # Loop through different number of neighbors
  pred.Yval = knn.cv(train=Xtrain, cl=Ytrain, k=i) # Predict on the left-out validation set
  validation.error =c(validation.error, mean(pred.Yval!=Ytrain)) # Combine all validation errors 
  }

numneighbor = max(allK[validation.error == min(validation.error)]) 

numneighbor
```
Using LOOCV, we have determined the optimal number of neighbors is `r numneighbor`. Now, we will rerun the analysis on the test data using `r numneighbor` nearest neighbors.

```{r KNN.actual}
set.seed(67)

pred.YTest = knn(train=Xtrain, test=Xtest, cl=Ytrain, k=numneighbor)

conf.matrix = table(predicted=pred.YTest, true=Ytest) 
conf.matrix

sum(diag(conf.matrix)/sum(conf.matrix))
```

KNN gives us a slightly higher prediction rate than logistic regression.


# Decision Trees by Mikey Boi
```{r}
michael.dat = danny.dat %>% 
  mutate(expansion1=log(expansion1+1),
         decade = 10 * (year_published %/% 10),
         above7 = as.numeric(ifelse(average_rating >= 7, 1 , 0))
         ) %>% 
  select(c(above7,max_playtime,min_age,min_players, max_players, designer, mechanic1,users_rated,expansion1,decade)) %>% 
  #dummy_cols(., remove_selected_columns = TRUE) %>%  #Couldn't get mechanic1 to register so have to do a trick
  mutate(above7=as.factor(above7)) %>% 
  drop_na()
```
I am going to add in a binary variable equal to one if the game was designed by a Top 10 designer. 

```{r}

top.ten.designer = michael.dat %>% filter(designer != "(Uncredited)") %>% count(designer, sort = T) %>% top_n(10, n)
top.ten.names = top.ten.designer$designer

#added in a top ten variable
michael.dat = michael.dat %>% mutate(top.ten = as.factor(ifelse(designer %in% top.ten.names, 1, 0))) %>% drop_na()
michael.dat = michael.dat %>% mutate_if(is.character, as.factor)

```
Testing and training data for myself.
```{r}
set.seed(4)

if (length(michael.dat$above7 %% 2 !=0)) {
  michael.dat = michael.dat[-c(round(runif(1, 1, length(michael.dat$above7)),0)),]
}
train.m = sample(1:nrow(michael.dat), 0.5*nrow(michael.dat))

train.dat.m<-michael.dat[train.m,]
test.dat.m<-michael.dat[-train.m,]
```

Now I am going to make the tree.
```{r}
library(tree)
tree.fit = tree(above7 ~., data = train.dat)
summary(tree.fit)
plot(tree.fit)
text(tree.fit, pretty = 0, cex = 0.7)
title("Classification Tree Built on Training Set")
```

Now to fit this to the testing data
```{r}
yhat.testset = predict(tree.fit, newdata = test.dat, type = 'class')

#Obtaining a confusion matrix
error.tree = table(yhat.testset, test.dat$above7)
tree.accuracy = sum(diag(error.tree)/sum(error.tree))
tree.error.rate = 1 - tree.accuracy
```

Hence, the tree had a misclassification rate of `r tree.error.rate`.

Now I will use cross validation to find the optimal tree size. 

```{r}
cv.for.tree = cv.tree(tree.fit, FUN=prune.misclass, K=10)
optimal.size = cv.for.tree$size[which.min(cv.for.tree$dev)]
```

THe optimal size for the tree is `r optimal.size`. 

Now to prune
```{r}
tree.prune = prune.misclass(tree.fit, best = optimal.size)
plot(tree.prune)
text(tree.prune, pretty = 0, cex = 0.8)
title("Pruned Tree Using CV for Optimal Size")
```

Predicting on the pruned tree:
```{r}
pred.prune = predict(tree.prune, newdata = test.dat, type = 'class')
error.prune = table(pred.prune, test.dat$above7)
prune.accuracy = sum(diag(error.prune))/sum(error.prune)
prune.error.rate = 1 - prune.accuracy
```

Therefore, the prune test error is `r prune.error.rate`. This looks to be very slightly better off than the non-pruned tree.

# Random Forests

Now to try random forests.
```{r}
library(randomForest)
train.dat = train.dat %>% mutate_if(is.character, as.factor)
test.dat = test.dat %>% mutate_if(is.character, as.factor)
rf.tree = randomForest(above7 ~., data = train.dat, mtry = 3, ntree = 500, importance = T)

plot(rf.tree)
```


Now going to get the test error rate for the random forest.
```{r}
yhat.rf = predict(rf.tree, newdata = test.dat)
rf.err = table(pred = yhat.rf, truth = test.dat$above7)
rf.err
rf.test.error = 1 - sum(diag(rf.err))/sum(rf.err)
```

Hence, the random forest got a test error rate of `r rf.test.error`. This is substantially better than anything we have tried before.

```{r}
importanceplot = varImpPlot(rf.tree)
importanceplot = as.data.frame(importanceplot)
```

Now running a loop to find the best $m$.
```{r}
# rf.error.loop = rep(0, 8)
# for (i in 2:9) {
#   rf.tree.loop = randomForest(above7 ~. -designer, data = train.dat.m, mtry = i, ntree = 500, importance = T)
#   yhat.rf.loop = predict(rf.tree.loop, newdata = test.dat.m)
#   rf.err.loop = table(pred = yhat.rf.loop, truth = test.dat.m$above7)
#   rf.test.error.loop = 1 - sum(diag(rf.err.loop))/sum(rf.err.loop)
#   rf.error.loop[i-1] = rf.test.error.loop
# }
```

From this the best m looks to be $m=2$ or $m=3$. In addition, the bagging test error rate (i.e. when $m=\rho$) does worse.






