---
title: "No More Bored Games: Predicting Which Board Games Get Highly Rated"
author: "Michael Topper (5788708) and Danny Klinenberg (7626054)"
output: 
  pdf_document:
    fig_caption: TRUE
    fig_width: 6
    fig_height: 3
bibliography: references.bib
editor_options: 
  chunk_output_type: console
header_includes:
  \usepackage{float}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,  message = FALSE, cache = TRUE, warning = FALSE, fig.pos = "H")
library(tidyverse)
library(fastDummies)
library(ROCR)
library(class)
library(ggpubr)
library(kableExtra)
library(memisc)
library(expss)
#library(rpart)
#library(partykit)
df = board_games <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-03-12/board_games.csv")
select = dplyr::select
theme_set(theme_bw())
```

```{r data.clean}
danny.dat <- df %>%
  separate(.,mechanic, "mechanic1",sep=",",extra="drop") %>% 
  mutate(expansion1=ifelse(is.na(expansion),0,str_count(df$expansion,",")))

top.ten.designer = danny.dat %>% filter(designer != "(Uncredited)") %>% count(designer, sort = T) %>% top_n(10, n)
top.ten.names = top.ten.designer$designer

#added in a top ten variable
danny.dat = danny.dat %>% mutate(top.ten = as.factor(ifelse(designer %in% top.ten.names, 1, 0)))
danny.dat.use<-danny.dat %>% 
  mutate(expansion1=log(expansion1+1),decade = 10 * (year_published %/% 10), above7 = as.numeric(ifelse(average_rating >= 7, 1 , 0))
         ) %>% 
  select(c(above7,max_playtime,min_age,min_players,max_players,top.ten,mechanic1,users_rated,expansion1,decade)) %>% 
  mutate(above7=as.factor(above7)) %>% 
  drop_na()



#Some mechanics have like 1 observation in them. I'm going to drop convert mechanics with less than 20 into just "other"
cat.mut<-danny.dat.use %>% 
  group_by(mechanic1) %>% 
  summarise(n=n()) %>% 
  filter(n<20)

danny.dat.use <- danny.dat.use %>% 
  mutate(mechanic1 = ifelse(mechanic1 %in% cat.mut$mechanic1, "other",mechanic1))

```


```{r train.test}
set.seed(4)

train<-sample(seq(1,dim(danny.dat.use)[1]), .5*dim(danny.dat.use)[1], replace = FALSE)
test<-sample(seq(1,dim(danny.dat.use)[1])[-train], .5*length(seq(1,dim(danny.dat.use)[1])[-train]), replace = FALSE)


train.dat<-danny.dat.use[train,]
test.dat<-danny.dat.use[test,]

#DO NOT TOUCH THIS UNTIL THE VERY VERY END. 
validation.dat <- danny.dat.use[-c(train,test),]
validation.dat  = validation.dat %>% 
  mutate_if(is.character, as.factor) %>% 
  mutate(above7=as.numeric(above7)-1)


```

# Introduction

Social interaction is on the decline. According to recent a recent study using a nationally representative sample of U.S. adolescents and college students, researchers found that adolescents in the 2010s spent significantly less time socializing with their peers than their previous generation counterparts [@twenge2019less]. Specifically, college-bound high school seniors spent less than an hour a day engaging in in-person social interaction. This decline can largely be attributed to the rise of technology. 


The ubiquity of technology has been linked to hindering the collaborative benefits that social interaction contributes. For instance, one study found that non-digital playtime in children lead to increases in creative thinking [@hitron2018digital]. Therefore, with the technology giants constantly battling each other for every second of our attention, it is imperative that human-on-human interaction still plays a major role in everyday life. One of the classic ways to achieve this important interaction is with board games.

Board games suffered a significant reduction in sales upon the spread of the smartphone. However, this trend has recently been reversed [@jolin_2016]. To aid this reverse, we sought to predict whether board games will be rated highly by their users. This information would be crucial so that the board game developers can remain relevant, and human interaction can be salvaged. To answer whether we can predict "what makes a good board game" we utilized four different supervised machine learning techniques: logistic regression, k-nearest neighbors, decision trees, and random forests. Our main results found that the random forests had the lowest misclassification rate, and predicted nearly 85% of the validation data correctly. 

# Data

Our data set on board games comes from the Board Game Geek database. This data set was published on the R for Datascience Github page for their Tidy Tuesday--a weekly event in which a new public data set is posted for any user to perform analysis on. In particular, this data set features games with at least 50 ratings for games published between the years 1950 and 2016. Surprisingly, this results in 10,532 games. The data features information on the following important variables: a description of the game, maximum number of plays, maximum playtime, minimum age, minimum playtime, year published, artist used for the game art, category of the game, family of the game, average rating, number of users rated, and publisher. Overall, the raw data possesses 22 variables, although we only performed machine learning techniques using 9 covariates. Many of the variables in the raw were duplicated into "new" variable names, creating the illusion of more information than was truly available. For instance, there were two variables that included identical information but were labeled differently as "max playtime" and "playing time".  We excluded the duplicates from the analysis. 

## Exploratory Analysis

Before building our machine learning models, we first performed exploratory analysis. To begin, we plotted several scatter and box-and-whisker plots to visualize the relationship between our covariates and whether the game received a high average score. 

We first will highlight four main features: *Maximum Playtime (minutes)*, *Users Rated*, *Decade*, and *Number of Expansions*. We chose to use maximum playtime over minimum or average because we thought it was the most representative of the game playing experience. In addition, minimum and average are expected to be highly correlated with maximum, so including one would suffice for predictive power. *User Rated* refers to the number of individuals that rated the game. This serves as a proxy for popularity of the game. *Decade* was a transformed variable from *date released*. We assumed there were some sort of generational fixed effects involved in the evolution of board games hopefully captured by *Decade*. Finally, we transformed *Expansions* to *Number of Expansions*. Good games tend to have more expansions than bad games so the magnitude was an appealing feature. The relationship was initially exponential; a few games had many types of expansions and were rated very high. To account for this, the number of expansions was log-transformed. This created a linear relationship for analysis.

```{r features_graphs_maker}
playtime<-df %>% filter(max_playtime < 10000) %>% 
  ggplot(aes(x = max_playtime, y = average_rating)) +
  geom_point(color = 'grey') +
  geom_smooth( se = F) +
  xlab("Maximum Playtime (minutes)") +
  ylab("")+
  theme(plot.title = element_text(hjust = 0.5)) 

user_rate<-df %>% ggplot(aes(x = users_rated, y = average_rating)) +
  geom_point(color = 'grey') +
  geom_smooth(se = F) +
  xlab("Users Rated") +
  ylab("")+
  theme(plot.title = element_text(hjust = 0.5)) 

year <- df %>% mutate(decade = 10 * (year_published %/% 10)) %>% 
  group_by(decade) %>% summarise(min_average_players = mean(min_players), avgdecaderating = mean(average_rating)) %>%
  ggplot(aes(x = decade, y = avgdecaderating)) + 
  geom_point() + 
  geom_smooth(method = 'lm', se = F) +
  xlab("Decade") +
  ylab("")+
  theme(plot.title = element_text(hjust = 0.5))  

expansion <- danny.dat %>%
  ggplot(., aes(I(log(expansion1 + 1)), average_rating)) +
  geom_point() +
  geom_smooth(se = F) +
  ylab("")+
  xlab("ln(Number Expansions)") 

```

```{r, fig.cap = "Relationship of Sample Features", fig.width=6, fig.height=3}
annotate_figure(ggarrange(playtime,user_rate,year, expansion),
  left = text_grob("Average Ratings", rot = 90)
)
```


```{r decade_graph, fig.width=6, fig.height=3, include=FALSE}
df %>% mutate(goodrating = as.factor(ifelse(average_rating >= 7, 1, 0))) %>% 
  ggplot(aes(x = goodrating, y = min_age, fill = goodrating)) +
  geom_boxplot() +
  labs(title = "Minimum Age and Rating", fill = "1 if rating above 7") +
  xlab("Classifier of Good or Bad Game") +
  ylab("Minimum Age Requirement") +
  theme(plot.title = element_text(hjust = 0.5)) 
```


```{r}
mechanics = df %>% separate_rows(mechanic, sep = ",") %>% select(mechanic, game_id, publisher)
```



In addition, we also created several other graphs to get a better feel for the data. Some of the questions we were interested in answering were: 

\begin{enumerate}
    \item \textit{Were there any popular designers in each decade and did these designers have an effect on the rating?}
    \item \textit{What were the most popular types of board games and does type effect the rating?}
\end{enumerate}

```{r designers, fig.cap="\\label{designer}Top 5 Designers by Decade", fig.width=6, fig.height=7}
df %>% mutate(decade = 10 * (year_published %/% 10)) %>% 
  filter(decade > 1950) %>% 
  filter(designer != '(Uncredited)') %>% 
  group_by(decade) %>% 
  count(designer, sort = T) %>%  
  filter(!is.na(designer)) %>% 
  arrange(desc(n)) %>% 
  top_n(5, n) %>% 
  ggplot(aes(x = designer, y = n, fill = as.factor(decade) )) +
  geom_col() +
  facet_wrap(~decade) +
  coord_flip() +
  labs(fill = "Decade") +
  xlab("") +
  ylab("Count")

```

Figure \ref{popular} shows the distribution of board game types. From this, we hypothesized that board game type plays a role in score of a game. For instance, card games may score systematically higher than adventure games. However, an initial concern was that there were many games that had very specific types. For example, *Connection Games* had 19 observations. This would lead to issues in cross validation. Because of this, we grouped all categories under a threshold as *Other*. Furthermore, we can see from Figure \ref{designer} that in the 1990s-2010s, a few designers were extremely prominent in the game designing business. Because of this, we suspected that a top designer has a role in the prediction of whether a game received a high average score. Hence, we created a binary variable equal to 1 if the designer was a top ten designer (by number of games designed) and 0 otherwise. More detailed explanation on this is provided in the next section. 

```{r pop_type, fig.cap="\\label{popular}Most Popular Types of Board Games", fig.width=6, fig.height=3}
catvars = df %>% 
  select(game_id, name, family, expansion, category, artist, designer, average_rating) %>% 
  gather(type, value, -game_id, -name) %>% 
  filter(!is.na(value)) %>% 
  separate_rows(value, sep = ',') %>% 
  arrange(game_id)

catcounts = catvars %>% 
  count(type, value,  sort = T)

catcounts %>% 
  filter(type == 'category') %>% 
  mutate(value = fct_reorder(value, n)) %>% 
  top_n(20, n) %>% 
  ggplot(aes(value, n)) +
  geom_col(fill = 'blue') +
  coord_flip() +
  ylab("Total") +
  xlab("Board Game Type") +
  theme(plot.title = element_text(hjust = 0.5)) 
```


## Final Predictors

For our final analysis, we utilized 9 final predictors. The predictors are a mix of both categorical and numerical data:
\begin{enumerate}
    \item Maximum playtime (max\_playtime)
    \item Minimum age (min\_age)
    \item Minimum players (min\_players)
    \item Maximum players (max\_players)
    \item Whether the game was designed by a ``top ten'' designer (top.ten)
    \item Game mechanics (mechanic1)
    \item Whether the game was an expansion (expansion1)
    \item Decade the game was published (decade)
    \item Number of users who rated the game (users\_rated)
\end{enumerate}
We decided on these predictors due to the relationships we saw in our exploratory analysis. Furthermore, the other variables were either too sparse or required advanced methods to clean. For instance, our compilation predictor which included information on whether or not the board game was part of a compilation series, only had information for 337 of our 10,000 observations. Therefore, we dropped this variable from analysis. Moreover, the variables "description", "image", and "thumbnail" required image processing techniques in which we believed were out of the scope of this course. However, we will note that each of these variables (if cleaned properly) could easily increase our prediction rate since advertising has a large effect on consumer behavior [@ackerberg2001empirically]. 

We created two new variables of the nine that were used as final predictors: a binary variable for whether the game was designed by a top ten designer, and decade the game was published. Originally, the data had information on the designer that created each game. To create the binary variable, we first filtered the data to make sure there were no uncredited games. Next, we counted the top designers and sorted them in descending order. We took the top 10 designers by the number of games they designed--if there was a tie (e.g. the 10th and 11th designer had the same count) we included them both as a top ten designer. For the decade variable, we simply grouped our year data into decades. This was accomplished because we hypothesized that games changed more by decade than by year. 

# Methods

To begin, we first made a classifier for whether a game had an average user score of a 7 or over on a 1-10 scale. This decision was made because 7 was approximately the 80th percentile of the distribution of scores (see Figure \ref{dist}).  After defining these classifiers, we performed logistic regression, k-nearest neighbors, and decision trees as our non-ensemble methods. For ensemble methods, we used random forests and bagging. More detailed explanations on how these were performed follow. Each of these techniques were pre-determined in our research proposals, thus excluding the possibility of us exhausting all methods to find the best choice by random chance.

To ensure replication of results, we set the seed to 4. We randomly split the data into a training, testing, and validation set. The splits were made randomly on a 50-25-25 split respectively. This equates to `r dim(train.dat)[1]` observations in the training, `r dim(test.dat)[1]` observations in the test and `r dim(validation.dat)[1]` observations in the validation set. The random splits were crucial as it allowed us to omit any sort of bias in non-random splits. Hence, we can expect that there are no systematic differences between our three sets. 

```{r, outcum_hist, fig.cap="\\label{dist}Distribution of Average Ratings Across Games"}
df %>% ggplot(aes(average_rating)) +
  geom_histogram(fill = "grey") +
  geom_vline(aes(xintercept = 7), color = 'red') +
  labs(xintercept = "Classification Cutoff") +
  theme(plot.title = element_text(hjust = 0.5))  +
  ylab("Count") +
  xlab("Average Rating") +
  theme_classic()
```
 
## Logistic Regression

To begin, we ran a logistic regression using the variables described above. The training rate versus cutoff is displayed below. The formula is:

$$Pr(Above=1|X)=\frac{e^{X^T\beta}}{1+e^{X^T\beta}}$$

```{r}
mod_logit<-glm(above7~.,data = train.dat, family = binomial(link = "logit"))
summ<-summary(mod_logit)
a<-summ$coefficients

```

where $X^T$ is the list of variables described before. Highly significant features in the training set (defined as having a p-value less than .001) are: **`r row.names(a)[which(a[,4]<.001)]`**. A full regression breakdown can be found at the end of the analysis.




```{r train.logit}
y.hat <- predict(glm(above7~.,data = train.dat, family = binomial(link = "logit")), type = "response")
cutoff <- function(p){
  return(as.numeric(y.hat > p))
}
```

The predicted values of a logistic regression are probabilities. This means that a cutoff has be chosen for *above7*. An immediate thought may be 50%. That way half the values are below or above. However, the cutoff should be chosen to minize the error rate. Figure \ref{cutoff} is a visual representation of accuracy versus cutoff values for the training set.

```{r train.cuttoff.graph, fig.cap="\\label{cutoff}Cutoff vs % Correct: Training", fig.width=6, fig.height=3}
hold <- tibble("cutoff" = seq(0,1,.01),
             "% correct" = rep(NA, length(seq(0,1,.01))))

for (i in seq_len(length(seq(0, 1, .01)))) {
  hold[i,2] <- mean(cutoff(hold$cutoff[i]) == train.dat$above7)
}

ggplot(hold, aes(x = cutoff, y = `% correct`)) +
  geom_line() +
  xlab("Cutoff") +
  ylab("% Correct")

```

Notice that the accuracy rate peaks between 25% and 50%. This means that we expect the cutoff rate for the test set to be near the same.

Next, we checked the optimal cutoff for the test set using the model created by the training model.


```{r Logit_ROC_AUC, fig.cap="ROC Curve", include=FALSE}
glm.fit <- glm(above7~.,data = train.dat, family = binomial(link = "logit"))
prob.training <- predict(glm.fit, type = "response")

prob.test <- round(predict(glm.fit, test.dat, type = "response"),digits = 5) 



pred <- prediction(prob.training, train.dat$above7)
perf <- performance(pred, measure = "tpr",x.measure = "fpr")
plot(perf, col = 2, lwd = 3)
abline(0,1)
```

A useful way of determining the quality of a fit is the ROC curve. The ROC curve shows the Type 1 and Type II errors of each cutoff for a logistic regression.


```{r AUC_val}
#AUC value
auc <- performance(pred, "auc")@y.values
```

The area under the curve is `r round(auc[[1]],3)`. The perfect fit would have an area under the curve of 1. This would mean the model perfectly predicts. Anything above .9 is considered great and anything above .75 is considered good. This is used to tell us if our model does well in general. With this knowledge, we proceeded to pick the optimal cutoff.

Optimal cutoff was chosen by minimizing the type I and type II error. Figure \ref{roc} shows this relationship:

```{r, fig.cap="\\label{roc}Test Error vs Threshold"}
# Obtaining best cutoff value:

fpr = performance(pred, "fpr")@y.values[[1]] 
cutoff = performance(pred, "fpr")@x.values[[1]] # FNR 
fnr = performance(pred,"fnr")@y.values[[1]]


matplot(cutoff, cbind(fpr,fnr), type = "l",lwd = 2, xlab = "Threshold",ylab = "Error Rate") # Add legend to the plot 
#legend("topright",legend = c("False Positive Rate","False Negative Rate"), col = c(1,2), lty = c(1,2))


rate = as.data.frame(cbind(Cutoff = cutoff, FPR = fpr, FNR = fnr)) 
rate$distance = sqrt((rate[,2])^2 + (rate[,3])^2)

index = which.min(rate$distance) 
best = rate$Cutoff[index] 

best.pred <- mean(as.numeric(prob.test > best) == test.dat$above7)
```

The red dotted line is the *false negative* rate while the black solid line is the *false positive* rate. The optimal threshold is chosen by minizing the Euclidean distance of the two curves from 0.

The best value to use as a cutoff was `r round(best,3)` with a  success rate of `r round(best.pred,3)`. This was the best cutoff value using the test set.

## KNN

*K Nearest Neighbors (KNN)* predicts by matching known observations with an observation we want to predict. KNN works very intuitively: it classifies data points by using classified data points that are "closest" to it. For example, if an unclassified data point is closest to three data points, two of which are classified as 1 and one of which is classified as 0, then the unclassified point would be classified as a 1 (majority rules). The difficulty of KNN is choosing the optimal number of neighbors, as the number of neighbors used to classify can drastically change the classification. The key is to minimize the MSE by choosing the optimal number of k. To begin, we ran the algorithm using k=2.

```{r KNN.setup}
Ytrain <- train.dat$above7
Xtrain <- train.dat %>% 
  select(-above7) %>% 
  mutate(top.ten = as.numeric(top.ten)) %>% 
  dummy_cols(., select_columns = "mechanic1",  remove_selected_columns = TRUE)
Xtrain <- scale(Xtrain,center = TRUE, scale = TRUE)

Ytest <- test.dat$above7
Xtest <- test.dat %>% 
  select(-above7) %>% 
  mutate(top.ten = as.numeric(top.ten)) %>% 
  dummy_cols(., select_columns = "mechanic1", remove_selected_columns = TRUE) %>% 
  scale(center = TRUE, scale = TRUE)

```

```{r KNN.train.2}
pred.ytrain <- knn(train = Xtrain, test = Xtrain, cl = Ytrain, k = 2)

conf.train <- table(predicted = pred.ytrain,observed = Ytrain)
```

```{r KNN.test.2}
pred.ytest <- knn(train = Xtrain, test = Xtest, cl = Ytrain, k = 2)

conf.test <- table(predicted = pred.ytest,observed = Ytest)
```

The training error with 2 neighbors was `r sum(diag(conf.train)/sum(conf.train))`. The true positive rate was `r round(conf.train[1,1]/sum(conf.train[,1]),3)` and the false positive rate is `r round(conf.train[1,2]/sum(conf.train[1,]),3)`.

The test error with 2 neighbors was `r sum(diag(conf.test)/sum(conf.test))`. The true positive rate was `r round(conf.test[1,1]/sum(conf.test[,1]),3)` and the false positive rate was `r round(conf.test[1,2]/sum(conf.test[1,]),3)`.

```{r KNN.cv, cache=TRUE}
validation.error <- NULL
allK <- 1:35
set.seed(66)

for (i in allK) { # Loop through different number of neighbors
  pred.Yval = knn.cv(train = Xtrain, cl = Ytrain, k = i) # Predict on the left-out validation set
  validation.error = c(validation.error, mean(pred.Yval != Ytrain)) # Combine all validation errors 
  }

numneighbor = max(allK[validation.error == min(validation.error)]) 

```

However, the odds of $k=2$ being the optimal number of neighbors was very slim. In order to determine the optimal number of neighbors, Leave One Out Cross Validation (LOOCV) was utilized. LOOCV works in the following steps:

\begin{enumerate}
    \item Remove one observation as a "holdout"
    \item Create a KNN model using the data exluding the "holdout"
    \item Predict using the models on the one observation holdout
    \item Record the error for each model
    \item Repeat for each observation
    \item Choose the number of neighbors (k) that minimized average error rate.
\end{enumerate}

This could be graphed, however, visually identifying the optimal number of neighbors is near impossible. Instead, we resulted to identifying the number of neighbors $k$ that minimized the error rate.
```{r neibor_graph, fig.cap="Neighbor Graph"}
kn_graph <- tibble(
  "Neighbors" = allK,
  "Error Rate" = validation.error
)
ggplot(kn_graph, aes(x = Neighbors, y = `Error Rate`)) +
  geom_line()

```

The optimal number of neighbors determined using LOOCV wass `r numneighbor`. Using the optimal number of neighbors, we produced the test confusion matrix and test error rate as shown in Table 1. 

```{r KNN.actual}
set.seed(67)

pred.YTest = knn(train = Xtrain, test = Xtest, cl = Ytrain, k = numneighbor)

conf.matrix = table(predicted = pred.YTest, true = Ytest) 

kable(conf.matrix, caption = "Test Confusion Matrix", align = c("c","c"), col.names = c("0","1")) %>% 
  kable_styling(bootstrap_options = "striped",  font_size = 8, latex_options = "hold_position") 
```

KNN returned a test error of `r round(sum(diag(conf.matrix)/sum(conf.matrix)),3)` with `r numneighbor` neighbors. This test error was approximately the same as the logistic regression.

## Decision Trees

Decision trees were made using the tree library in R. To begin, we built the tree including all predictors. Using the training data, our prediction tree is diplayed in Figure \ref{DecisionTree}.

Decision trees are built using iteration through the predictors. Specifically, a decision tree first begins by choosing a predictor and calculating the amount of correctly classified observations. After doing this, it calculates impurity for the predictor. Impurity is essentially a weighted average of the misclassification rate. After all predictors are iterated through, the predictor with the lowest impurity is chosen. This process is repeated multiple times until there are no further predictors to iterate through. 

```{r treegraph, fig.cap="\\label{DecisionTree}Decision Tree on Training Set", fig.width=6, fig.height=4}
library(tree)
tree.fit = tree(above7 ~., data = train.dat)
plot(tree.fit)
text(tree.fit, pretty = 0, cex = 0.7)
title(" ")
```

```{r}
yhat.testset = predict(tree.fit, newdata = test.dat, type = 'class')
#Obtaining a confusion matrix
error.tree = table(yhat.testset, test.dat$above7)
tree.accuracy = sum(diag(error.tree)/sum(error.tree))
tree.error.rate = 1 - tree.accuracy
```
This particular tree did an adaquet job predicting whether a game was rated highly--it received a misclassification rate of `r round(tree.error.rate,3)`. To read the tree, you need to start at the top of the tree (the "root" node) and follow the tree downwards based on the criteria given. If the criteria is true, follow the tree to the right, if the criteria is false, follow the tree left.


```{r}
cv.for.tree = cv.tree(tree.fit, FUN=prune.misclass, K=10)
optimal.size = cv.for.tree$size[which.min(cv.for.tree$dev)]
```

```{r prunetree, fig.cap="\\label{Prunetree}Pruned Decision Tree on Test Data Using Cross Validation for Optimal Size", fig.width=6, fig.height=4}
tree.prune = prune.misclass(tree.fit, best = optimal.size)
plot(tree.prune)
text(tree.prune, pretty = 0, cex = 0.8)
title(" ")

```
```{r}
pred.prune = predict(tree.prune, newdata = test.dat, type = 'class')
error.prune = table(pred.prune, test.dat$above7)
prune.accuracy = sum(diag(error.prune))/sum(error.prune)
prune.error.rate = 1 - prune.accuracy
```



To enhance this tree, we used cross validation to find the optimal tree size. After performing cross validation, we found that the optimal tree size was `r optimal.size`.  Using this optimal size, we pruned the tree and once again performed our tree prediction algorithm. The idea behind pruning is to avoid overfitting the data so that the algorithm will perform better on the testing data. When pruning and optimal tree size were included, the test misclassication rate was `r round(prune.error.rate, 3)`. This was extremely similar to our original tree and therefore we can conclude that pruning/cross validation did not lead to a significant increase in performance. Our pruned decision tree is shown in Figure \ref{Prunetree}.

## Random Forest/Bagging

Our main ensemble method used was the random forest. The random forest combines the idea of bootstrapping and decision trees to make predictions. Specifically, a bootstrap data set is first created. This means that we draw a random sample of observations from our data with replacement. The replacement aspect is crucial here--we need to be certain that our bootstrap data set is not identical to the original data set. After the data is bootstrapped, a decision tree is made using the bootstrapped data. However, the twist is that only a random subset of predictors are considered at each step. In particular, we denote this random subset of predictors as the $m$ parameter. This process is repeated multiple times (in our case 500 for computing time purposes) to create the random forest. We initialized our random forest by arbitrarily choosing the parameter $m= 3$.
 
```{r , include = FALSE}
library(randomForest)
train.dat = train.dat %>% mutate_if(is.character, as.factor)
test.dat = test.dat %>% mutate_if(is.character, as.factor)
rf.tree = randomForest(above7 ~., data = train.dat, mtry = 3, ntree = 500, importance = T)
oobData = as.data.table(plot(rf.tree))

# Define trees as 1:ntree
oobData[, trees := .I]

melt = reshape2::melt
# Cast to long format
oobData2 = melt(oobData, id.vars = "trees")
setnames(oobData2, "value", "error")


```

```{r rferror, fig.cap="\\label{rferror}Error of the Random Forest", fig.width=6, fig.height=3}
oobData2 %>% 
  ggplot(aes(x = trees, y = error, color = variable)) +
  geom_line() +
  xlab("Trees") +
  ylab("Error") +
  labs(color = "")
```


```{r}
yhat.rf = predict(rf.tree, newdata = test.dat)
rf.err = table(pred = yhat.rf, truth = test.dat$above7)
rf.test.error = 1 - sum(diag(rf.err))/sum(rf.err)
```


```{r, include = FALSE}
## I don't want this to show up!
imp = varImpPlot(rf.tree)
imp = as.data.frame(imp)
imp$varnames = rownames(imp)
rownames(imp) = NULL
```

```{r importanceplot, fig.cap="\\label{importanceplot}Importance Plot", fig.width=6, fig.height=3}

## I want to put these side by side
mean.accuracy = imp %>% 
  ggplot(aes(x = reorder(varnames, MeanDecreaseAccuracy), y = MeanDecreaseAccuracy)) +
  geom_point() +
  geom_segment(aes(x = varnames, xend=varnames, y = 0, yend = MeanDecreaseAccuracy)) +
  xlab("Variable") +
  ylab("Mean Decrease Accuracy") +
  coord_flip()

mean.gini = imp %>% 
  ggplot(aes(x = reorder(varnames, MeanDecreaseGini), y = MeanDecreaseGini)) +
  geom_point() +
  geom_segment(aes(x = varnames, xend=varnames, y = 0, yend = MeanDecreaseGini)) +
  xlab("Variable") +
  ylab("Mean Decrease Gini") +
  coord_flip()

ggarrange(mean.gini, mean.accuracy)
```


```{r}
rf.error.loop = rep(0, 8)
for (i in 2:9) {
  rf.tree.loop = randomForest(above7 ~., data = train.dat, mtry = i, ntree = 500, importance = T)
  yhat.rf.loop = predict(rf.tree.loop, newdata = test.dat)
  rf.err.loop = table(pred = yhat.rf.loop, truth = test.dat$above7)
  rf.test.error.loop = 1 - sum(diag(rf.err.loop))/sum(rf.err.loop)
  rf.error.loop[i-1] = rf.test.error.loop
}
bagging.error = rf.error.loop[8]
optimal.m = which(rf.error.loop == min(rf.error.loop)) +1
```



When $m= 3$ we found that we found that our misclassification rate was `r round(rf.test.error, 3)`. This was a significant upgrade from our previous decision tree method. Of course, this is expected with all ensemble methods: ensemble methods possess the unique property that they will be as good as or better than their subcomponents. By observing Figure \ref{importanceplot} we can see that the predictors which were most important in our construction of the random forest were *users rated*, *decade*, *game mechanics (mechanic1)*, and *maximum playtime*. This is not surprising as these correlate well with what we predicted in our exploratory analysis.

To find the optimal $m$, we looped over multiple values of $m$. Note that when $m=9$, we are performing bagging. Using our results, we find that the test error for bagging is `r round(bagging.error,3)`. Observe that Table 2 shows the different test error rates for the different values of $m$. From the table, we find that the best value for $m$ that minimizes our test error is `r round(optimal.m,3)`.  
```{r mtry, fig.cap="\\label{mtry}Importance Plot", fig.width=6, fig.height=3}
mtry.table <- tibble("m" = c("2","3","4","5","6","7","8","9"), "Test Errors" = round(rf.error.loop, 3))

kable(mtry.table, caption = "Parameter m and Test Errors", align = c("c","c")) %>% 
  kable_styling(bootstrap_options = "striped",  font_size = 8, latex_options = "hold_position") %>% 
    row_spec(0, bold = T, background = col2hex("deepskyblue4"), color = "white")                   
                    
```



# Discussion

We performed four machine learning methods to predict which board games will score above a 7. We first observed the data, identified key variables, and tranformed when necessary. We then split the data into training (50%), testing (25%), and validation (25%). Finally, we took all of our optimal models from our analysis and tested how well they predicted using the validation data. The results are shown in Table 3.

```{r VAlidation_make}
#Logit

logit.valid <- round(predict(glm.fit, newdata = validation.dat, type = "response"),digits = 5) 
logit.pred <- as.numeric(logit.valid > best)
logit_error <- mean((validation.dat$above7 != logit.pred))

#KNN

Yvalid <- validation.dat$above7
Xvalid <- validation.dat %>% 
  select(-above7) %>% 
  mutate(top.ten = as.numeric(top.ten)) %>% 
  dummy_cols(., select_columns = "mechanic1", remove_selected_columns = TRUE) %>% 
  scale(center = TRUE, scale = TRUE)

pred.ytest.val <- knn(train = Xtrain, test = Xvalid, cl = Ytrain, k = numneighbor)
KNN_error <- mean(pred.ytest.val != Yvalid)

# Tree

pred.prune.val = predict(tree.prune, newdata = validation.dat, type = 'class')
error.prune.val = table(pred.prune.val, validation.dat$above7)
prune.accuracy.val = sum(diag(error.prune.val))/sum(error.prune.val)
tree_error = 1 - prune.accuracy.val

# Random Forest

yhat.rf.v = predict(rf.tree, newdata = validation.dat)
rf.err.v = table(pred = yhat.rf.v, truth = validation.dat$above7)
rf_error = 1 - sum(diag(rf.err.v))/sum(rf.err.v)

validation_table <- tibble("Method" = c("Logistic","KNN","Tree","Random Forest"),
                         "Validation Error Rate" = round(c(logit_error,KNN_error,tree_error,rf_error),3)
)

kable(validation_table, caption = "Validation Error Rate", align = c("c","c")) %>% 
  kable_styling(bootstrap_options = "striped",  font_size = 8, latex_options = "hold_position") %>% 
    row_spec(0, bold = T, background = col2hex("deepskyblue4"), color = "white")
```

For this dataset, logistic performed the worst. KNN and tree performed relatively similar while Random Forest was the clear winner. Given this dataset, we would recommend using Random Forest to predict future quality board games. The major benefit of Random Forest is the ensemble nature: by aggregating many trees the method is able to add precision to out of sample modeling. Our random forest utilized all 9 of our predictors and was created using the parameter $m=3$ with the number of trees equal to 500. We suspect that with more predictors, we could have succeeded in building a more powerful model since our data was limited to only 9 predictors that were useable. However, predicting nearly 85% of the validation data correctly is an astounding feat for our first machine learning project. For future research, we could potentially increase our classification rate by cleaning the description and image data that came with the data set. As mentioned, cleaning this data and putting it into a form the algorithm could interpret was beyond the scope of this course, but we believe that they could greatly enhance our model. One could imagine that consumers value certain shapes or colors that greatly correspond to their user rating. However, until we unravel these techniques, all we can do is play on.



# Conclusion

Using data from the Board Game Geek database which contained information on over 10,000 games, we used machine learning techniques to predict which board games were considered "good" by their users. "Good" was defined as a binary classifier equal to 1 if the game had an average user score of 7 or above on a scale of 1-10. After data cleaning, our data set included 9 predictors which we performed exploratory analysis on to find meaningful relationships in the raw data. We found many positive correlations between our predictors and the average user score. With this information in mind, we performed logistic regression, k-nearest neighbors, decision trees, and random forests to build a predictive model. Unsurprisingly, the random forest using $m=3$ provided the best predictions with nearly 85% of our validation data being correctly classified. We hope that our predictive model can be utilized by board game companies to better their customers and continue the revival of board games. As mentioned, board games are a way of promoting person-on-person interaction: a lacking feature of today's technology driven world. With our model, we can hope that we are mitigating the decline of human interaction.


# Extra: Graveyard of Failed Ideas

It is worth noting that this is our third iteration of this machine learning project. For our first project (our first proposal submitted), we wanted to NYC restaurant health inspections to find out which restaurants close and what are the best predictors of closing. This project was scrapped because it needed additional data from Yelp on restaurant closers. The Yelp data set proved to be hard to obtain and merge, so we pivoted to a second project. Our second project (which we submitted yet another proposal for) was using IPUMS/CPS data to predict what types of people get divorced. This seemed like an excellent idea, as we had easily accessible data that was relatively clean. After doing exploratory analysis and cleaning, we were left with very little predictive power: only five predictors contained ample data to perform analysis on. When we ran this data through our algorithms, the algorithms would not give any results because power was too low. In fact, the decision tree only contained one branch. Because of this, we needed to find a new project in very little time (this occurred only a week and a half ago). We settled on this idea because the data was relatively clean, large, and we KNEW we could get some predictions. Since our final predicts around 85% of the validation data correctly, we can say that "third times a charm". 

# Appendix

## Extra Code

```{r logit_regression}
mtable(mod_logit)
```

# References